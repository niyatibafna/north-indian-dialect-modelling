{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "better-galaxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting cleaned data in one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "herbal-publicity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"../\")\n",
    "from crawled_data import CrawledData\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "import math\n",
    "import json\n",
    "import string\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "advisory-heaven",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting files for rajasthani\n",
      "Getting files for gujarati\n",
      "Getting files for magahi\n",
      "Getting files for awadhi\n",
      "Getting files for bajjika\n",
      "Getting files for chhattisgarhi\n",
      "Getting files for .DS_Store\n",
      "Getting files for braj\n",
      "Getting files for hindi\n",
      "Getting files for garhwali\n",
      "Getting files for bhojpuri\n",
      "Getting files for maithili\n",
      "Getting files for sanskrit\n",
      "Getting files for pali\n",
      "Getting files for angika\n",
      "Getting files for haryanvi\n",
      "Getting files for sindhi\n",
      "Getting files for marathi\n",
      "Getting files for nepali\n",
      "dict_keys(['rajasthani', 'gujarati', 'magahi', 'awadhi', 'bajjika', 'chhattisgarhi', 'braj', 'hindi', 'garhwali', 'bhojpuri', 'maithili', 'sanskrit', 'pali', 'angika', 'haryanvi', 'sindhi', 'marathi', 'nepali'])\n"
     ]
    }
   ],
   "source": [
    "data_obj = CrawledData()\n",
    "# data_obj.read_crawled_data(\"../data/HinDialect/crawled/folksongs/\", remove_punctuation = False)\n",
    "data_obj.read_crawled_data(\"../data/HinDialect/crawled/poetry/\", remove_punctuation = False)\n",
    "print(data_obj.data.keys())\n",
    "acc_data = dict()\n",
    "for lang in data_obj.data:\n",
    "    acc_data[lang] = \" \".join([data_obj.data[lang][file_id][\"text\"] for file_id in data_obj.data[lang]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "tropical-least",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HinDialect 1.0\n",
    "dev_range = range(2305, 2404)\n",
    "for lang in data_obj.data:\n",
    "    acc_data[lang] = \" \".join([data_obj.data[lang][file_id][\"text\"] for file_id in data_obj.data[lang]])\n",
    "    acc_data[lang] = \"\".join([c if ord(c) in dev_range else \" \" for c in acc_data[lang]])\n",
    "    acc_data[lang] = \" \".join(acc_data[lang].split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "optimum-oakland",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = \"../data/crawled_cleaned/\"\n",
    "for lang in data_obj.data:\n",
    "    filepath = DIR + lang + \".txt\"\n",
    "    with open(filepath, \"w\") as f:\n",
    "        f.write(acc_data[lang])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "rapid-sunrise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New version of HinDialect : 1.1\n",
    "## Removing bad letters:\n",
    "##  - Removing the \"wrong\" punctuation\n",
    "##  - Replacing multiple newlines by a single newline \n",
    "##  - Replacing all spaces with a single space\n",
    "## Reformatting into simple text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-probability",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_data = defaultdict(lambda: \"\")\n",
    "\n",
    "for lang in data_obj.data:\n",
    "    for file_id in data_obj.data[lang]:\n",
    "        acc_data[lang] += data_obj.data[lang][file_id][\"title\"].strip() + \"\\n\"\n",
    "        acc_data[lang] += data_obj.data[lang][file_id][\"text\"].strip(\"\\n\").strip() + \"\\n\"\n",
    "        # Add empty line to separate pieces\n",
    "        acc_data[lang] += \"\\n\" \n",
    "        \n",
    "        acc_data[lang] = \"\\n\".join([line.strip() for line in acc_data[lang].split(\"\\n\")])\n",
    "    \n",
    "    acc_data[lang].strip(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-viewer",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unicode normalization\n",
    "replacement = {\n",
    "    \"S\":\"ऽ\",\n",
    "    \"s\":\"ऽ\",\n",
    "    \"ऎ\":\"ऐ\",\n",
    "    \"ऒ\":\"ओ\",\n",
    "    \"ॊ\":\"ो\",\n",
    "    \"ऴ\":\"ऴ\",\n",
    "}\n",
    "remove = {\n",
    "    'I', 'S', 's', 'µ', 'î', 'ð', 'ù'\n",
    "}\n",
    "\n",
    "\n",
    "for lang, data in acc_data.items():\n",
    "    clean = \"\"\n",
    "    for c in data:\n",
    "        new_char = \"\"\n",
    "        if c in replacement:\n",
    "            new_char = replacement[c]\n",
    "        elif c not in remove:\n",
    "            new_char = c\n",
    "        clean += new_char\n",
    "    \n",
    "    acc_data[lang] = clean\n",
    "\n",
    "    \n",
    "## \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-reach",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iso_code(lang):\n",
    "    '''Returns ISO code of language'''\n",
    "    iso_codes = {\"angika\" : \"anp\",\n",
    "                 \"awadhi\" : \"awa\",\n",
    "                 \"baiga\" : \"mis\",\n",
    "                 \"bengali\" : \"ben\",\n",
    "                 \"bhadrawahi\" : \"bhd\",\n",
    "                 \"bhojpuri\" : \"bho\",\n",
    "                 \"braj\" : \"bra\",\n",
    "                 \"bhili\" : \"bhb\",\n",
    "                 \"bundeli\" : \"bns\",\n",
    "                 \"chhattisgarhi\" : \"hne\",\n",
    "                 \"garhwali\" : \"gbm\",\n",
    "                 \"gujarati\" : \"guj\",\n",
    "                 \"haryanvi\" : \"bgc\",\n",
    "                 \"himachali\" : \"mis\",\n",
    "                 \"hindi\" : \"hin\",\n",
    "                 \"kanauji\" : \"bjj\",\n",
    "                 \"khadi_boli\" : \"mis\",\n",
    "                 \"korku\" : \"kfq\",\n",
    "                 \"kumaoni\" : \"kfy\",\n",
    "                 \"malvi\" : \"mup\",\n",
    "                 \"magahi\" : \"mag\",\n",
    "                 \"marathi\" : \"mar\",\n",
    "                 \"nimadi\" : \"noe\",\n",
    "                 \"panjabi\" : \"pan\",\n",
    "                 \"rajasthani\" : \"raj\",\n",
    "                 \"sanskrit\" : \"san\",\n",
    "    }\n",
    "    return iso_codes[lang]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-clinton",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = \"../data/crawled_cleaned/poetry/\"\n",
    "if not os.path.exists(DIR):\n",
    "    os.mkdir(DIR)\n",
    "for lang in data_obj.data:\n",
    "    filepath = DIR + lang + \"-\" + get_iso_code(lang) + \".txt\"\n",
    "    with open(filepath, \"w\") as f:\n",
    "        f.write(acc_data[lang])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-decline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUGH WORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selected-british",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = \"../data/HinDialect/HinDialect 1.1/\"\n",
    "if not os.path.exists(DIR):\n",
    "    os.mkdir(DIR)\n",
    "for lang in data_obj.data:\n",
    "    filepath = DIR + lang + \"-\" + get_iso_code(lang) + \".txt\"\n",
    "    with open(filepath, \"w\") as f:\n",
    "        f.write(acc_data[lang])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-hopkins",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "downtown-priority",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from indicnlp.tokenize import indic_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exempt-moses",
   "metadata": {},
   "outputs": [],
   "source": [
    "from indicnlp.tokenize import indic_tokenize\n",
    "for lang in acc_data:\n",
    "    if \"\\n\\n\" in acc_data[lang]:\n",
    "        a = acc_data[lang].split(\"EOP\")\n",
    "        for a1 in a:\n",
    "            if \"\\n\\n\" in a1:\n",
    "                print(lang, a1)\n",
    "                print(\"END\")\n",
    "                print(\"NEW\")\n",
    "                a1 = \"\\n\".join([word for word in a1.split(\"\\n\") if word not in string.whitespace + \"\"])\n",
    "                a1 = \" \".join(indic_tokenize.trivial_tokenize(a1))\n",
    "                if \"\\n\\n\" in a1:\n",
    "                    print(\"Still present\")\n",
    "                else:\n",
    "                    print(\"not\")\n",
    "                print(a1)\n",
    "                print(\"TRUE END\")\n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "necessary-headquarters",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['सुकुवार', '।', '।', 'टेक']\n"
     ]
    }
   ],
   "source": [
    "print(indic_tokenize.trivial_tokenize(\"सुकुवार।।टेक\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caring-institution",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = \"bhojpuri\"\n",
    "a = Counter(acc_data[l].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "detected-gather",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14, 13, 13, 13, 12, 12, 12, 12, 12, 12]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = sorted([len(k) for k in a], reverse= True)\n",
    "lens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "olive-algorithm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ओएडेंगोएड़ें\n",
      "ओएड़ेगोएड़े\n",
      "गाँवचकुदरवा\n",
      "उदबासबेचैनी\n",
      "ब्रह्मपुत्र\n",
      "रामानुजरामानंद\n",
      "न्यारीप्यारी\n",
      "श्रीरामकॄष्ण\n",
      "बिक्रमार्जुन\n",
      "विश्वामित्र\n",
      "खुटुर-खुटुर\n",
      "बेदनेबेआकुल\n",
      "बेसरियानकबेसर\n",
      "फुफुतियाफांड\n",
      "कीड़ोंमकोड़ों\n",
      "छोड़ींछोड़ीं\n",
      "प्रश्नोत्तर\n",
      "धिरकारीरुदल\n",
      "लौंडेलपाड़न\n",
      "छिंगुनियाके\n",
      "नाँहींनाँहीं\n",
      "प्रतिबिंबनि\n",
      "हरिश्चन्द्र\n",
      "कोठियाअँटरिया\n",
      "भैयाभ‍उज‍इया\n",
      "पुतवापतोहिया\n",
      "ग‍इयाभँइसिया\n",
      "सुसुकिसुसुकि\n",
      "हारहिंडोलवा\n"
     ]
    }
   ],
   "source": [
    "for w in a:\n",
    "    if len(w) > 10:\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-russia",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
