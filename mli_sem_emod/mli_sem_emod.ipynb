{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-darwin",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "located-fellowship",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import argparse\n",
    "import math\n",
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "import editdistance\n",
    "import Levenshtein as lv\n",
    "import copy\n",
    "import random\n",
    "from pyfasttext import FastText\n",
    "\n",
    "class MLI_SEM_EMOD:\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.prev_trans_matrix = defaultdict(lambda:defaultdict(lambda:0))\n",
    "        self.trans_matrix = defaultdict(lambda:defaultdict(lambda:0))\n",
    "        self.totals = defaultdict(lambda:1)\n",
    "        \n",
    "        self.seen = defaultdict(lambda:set())\n",
    "        self.null_char = \"NULL\"\n",
    "        \n",
    "        self.init_same_char_counts = 1\n",
    "        self.init_total = 2\n",
    "        \n",
    "        self.training = False\n",
    "        \n",
    "        self.sem_nns = dict()\n",
    "        self.K = 50 #nearest neighbours taken into consideration\n",
    "\n",
    "\n",
    "    def get_args(self):\n",
    "        parser = argparse.ArgumentParser(description = \"Build a lexicon from 2 corpora using iterative OD.\")\n",
    "        parser.add_argument(\"--source_file\", type=str, default=None, required=True, help=\"Source corpus filepath\")\n",
    "        parser.add_argument(\"--target_file\", type=str, default=None, required=True, help=\"Target corpus filepath\")\n",
    "        parser.add_argument(\"--model\", type=str, default=None, required=True, help=\"fastText bilingual embeddings file\")\n",
    "        parser.add_argument(\"--max_lexicon_length\", type=int, default=math.inf, help=\"Maximum length of extracted lexicon\")\n",
    "        parser.add_argument(\"--min_source_freq\", type=int, default=None, help=\"Min freq of source side words\")\n",
    "        parser.add_argument(\"--min_target_freq\", type=int, default=None, help=\"Min freq of target side words\")\n",
    "        parser.add_argument(\"--iterations\", type=int, default=50, help=\"Maximum EM iterations\")\n",
    "        parser.add_argument(\"--batch_size\", type=int, default=100, help=\"Batch size for a single iteration\")\n",
    "        parser.add_argument(\"--updates\", type=int, default=10, help=\"Number of updates per iterations\")\n",
    "        parser.add_argument(\"--load_pretrained\", action=\"store_false\", help=\"Will load model parameters from PARAMS_DIR\")\n",
    "        parser.add_argument(\"--OUTPATH\", type=str, help=\"Path for saving lexicon (JSON)\")\n",
    "        parser.add_argument(\"--PARAMS_DIR\", type=str, help=\"Directory path for saving model parameters\")\n",
    "\n",
    "        return parser.parse_args()\n",
    "\n",
    "    def read_file(self, filepath):\n",
    "        '''Reads text file and returns as string'''\n",
    "        with open(filepath, \"r\") as f:\n",
    "            return f.read()\n",
    "        \n",
    "    def get_frequency_threshold(self, lang_lexicon):\n",
    "        '''Logarithmic freq threshold'''\n",
    "\n",
    "        total = sum(lang_lexicon.values())\n",
    "        return math.log(total, 100) - 1\n",
    "\n",
    "\n",
    "    def get_lexicon_words(self, source_words, target_words,\\\n",
    "                          min_source_freq, min_target_freq):\n",
    "        '''Decide which source-side words will be in the lexicon'''\n",
    "\n",
    "        cand_source_words = defaultdict(lambda:0, {w:f for w, f in source_words.items() if f >= min_source_freq})\n",
    "        cand_target_words = defaultdict(lambda:0, {w:f for w, f in target_words.items() if f >= min_target_freq})\n",
    "\n",
    "        return cand_source_words, cand_target_words\n",
    "    \n",
    "    \n",
    "    def initialize_trans_matrix(self, cand_source_words, cand_target_words, type = \"uniform\"):\n",
    "        '''Initialize transition matrix and totals matrix, including insertions and deletions'''\n",
    "        \n",
    "        print(\"Intializing TRANSITION MATRIX...\")\n",
    "        source_chars = set(\"\".join(word for word in cand_source_words))\n",
    "        source_chars.add(self.null_char)\n",
    "        target_chars = set(\"\".join(word for word in cand_target_words))\n",
    "        target_chars.add(self.null_char)\n",
    "        \n",
    "        total_targets = len(target_chars)\n",
    "\n",
    "        for source in source_chars:\n",
    "            for target in target_chars:\n",
    "                if source == target:\n",
    "                    self.trans_matrix[source][target] = self.init_same_char_counts\n",
    "                    continue\n",
    "                \n",
    "                if source in target_chars:\n",
    "                    self.trans_matrix[source][target] = \\\n",
    "                    (self.init_total-self.init_same_char_counts)/(total_targets-1)\n",
    "                    \n",
    "                else:\n",
    "                    self.trans_matrix[source][target] = \\\n",
    "                    self.init_total/total_targets\n",
    "        \n",
    "        print(\"Intialized TRANSITION MATRIX of dimensions {}x{}\".format(len(source_chars)+1, len(target_chars)+1))\n",
    "                \n",
    "    def initialize_model_params(self, iterations, batch_size, updates, \\\n",
    "                                cand_source_words, cand_target_words, \\\n",
    "                                load_pretrained_model = False, PARAMS_DIR = None):\n",
    "        '''Initializes training params'''\n",
    "        \n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.iterations = iterations\n",
    "        self.updates = updates\n",
    "        \n",
    "        if load_pretrained_model:\n",
    "            self.load_model_params(PARAMS_DIR)\n",
    "        else:\n",
    "            self.initialize_trans_matrix(cand_source_words, cand_target_words)\n",
    "        \n",
    "        #self.totals and self.seen are already correctly initialized\n",
    "    \n",
    "    \n",
    "    def op2chars(self, op, source, target):\n",
    "        '''Returns chars based on levenshtein op'''\n",
    "        \n",
    "        if op[0] == \"replace\":\n",
    "            char1 = source[op[1]]\n",
    "            char2 = target[op[2]]\n",
    "        if op[0] == \"insert\":\n",
    "            char1 = self.null_char\n",
    "            char2 = target[op[2]]\n",
    "        if op[0] == \"delete\":\n",
    "            char1 = source[op[1]]\n",
    "            char2 = self.null_char\n",
    "        if op[0] == \"retain\":\n",
    "            char1 = source[op[1]]\n",
    "            char2 = source[op[1]]\n",
    "        \n",
    "        return char1, char2\n",
    "    \n",
    "    def augmented_ops(self, source, target):\n",
    "        '''Returns minimal ed ops but also char retentions'''\n",
    "        ops = lv.editops(source, target)\n",
    "        bad_sidxs = {sidx for (op, sidx, _) in ops if op != \"insert\"}\n",
    "        ret_idxs = [(\"retain\", sidx) for sidx in range(len(source)) if sidx not in bad_sidxs]\n",
    "        return ops + ret_idxs\n",
    "\n",
    "\n",
    "    def update_params(self, pair):\n",
    "        '''Update matrix counts given a new observation'''\n",
    "        (source, target) = pair\n",
    "        ops = self.augmented_ops(source, target)\n",
    "        \n",
    "        for op in ops:\n",
    "            \n",
    "            char1, char2 = self.op2chars(op, source, target)\n",
    "            \n",
    "            self.trans_matrix[char1][char2] += 1\n",
    "            self.totals[char1] += 1\n",
    "            \n",
    "    \n",
    "    def update_params_all(self, pairs):\n",
    "        '''Update all paramaters for a collection of pairs'''\n",
    "        \n",
    "        self.prev_trans_matrix = copy.deepcopy(self.trans_matrix)\n",
    "        \n",
    "        for pair in pairs:\n",
    "            if pair[1] not in self.seen[pair[0]]:\n",
    "                self.seen[pair[0]].add(pair[1])\n",
    "                self.update_params(pair)\n",
    "        \n",
    "    \n",
    "    def check_convergence(self):\n",
    "        return self.trans_matrix == self.prev_trans_matrix\n",
    "\n",
    "    \n",
    "    def find_neg_log_prob(self, source, target):\n",
    "        '''Find log probability of source --> target using trans matrix'''\n",
    "        ops = self.augmented_ops(source, target)\n",
    "        log_prob = 0\n",
    "        for op in ops:\n",
    "            char1, char2 = self.op2chars(op, source, target)\n",
    "            try:\n",
    "                log_prob += math.log(self.trans_matrix[char1][char2]/self.totals[char1])\n",
    "            except:\n",
    "                continue\n",
    "#                 print(char1, char2)\n",
    "#                 print(source, target)\n",
    "#                 print(self.trans_matrix[char1][char2])\n",
    "#                 print(self.totals[char1])\n",
    "#                 print(self.trans_matrix[char1][char2]/self.totals[char1])\n",
    "                \n",
    "                \n",
    "        \n",
    "        return -log_prob\n",
    "    \n",
    "    def get_sem_candidates(self, word, cand_target_words, K = 50):\n",
    "        '''Get semantics-based candidates for a word based on bilingual embeddings'''\n",
    "        \n",
    "        if word not in self.sem_nns:\n",
    "            nns = self.model.nearest_neighbors(word, k = K)\n",
    "            self.sem_nns[word] = {nn for nn in nns if nn[0] in cand_target_words}\n",
    "        \n",
    "        if not self.training and word in cand_target_words:\n",
    "            self.sem_nns[word].add(word)\n",
    "        \n",
    "        return self.sem_nns[word]\n",
    "        \n",
    "    def find_best_match(self, source, cand_target_words, num_targets = 5):\n",
    "        '''Find best match for source over cand words'''\n",
    "        if self.training:\n",
    "            min_dist, best_word = math.inf, \"\"\n",
    "            for cand in cand_target_words:\n",
    "                if source == cand:\n",
    "                    continue\n",
    "                    dist_score = self.find_neg_log_prob(source, cand)\n",
    "                    if dist_score < min_dist:\n",
    "                        min_dist = dist_score\n",
    "                        best_word = cand\n",
    "            return source, best_word, min_dist\n",
    "        \n",
    "#         print(source in cand_target_words\n",
    "        cand_target_words.append(source)\n",
    "        dist_scores = [(cand, self.find_neg_log_prob(source, cand)) for cand in cand_target_words]\n",
    "        best_pairs = sorted(dist_scores, key = lambda x:x[1])[:num_targets]\n",
    "        \n",
    "        return source, best_pairs\n",
    "\n",
    "\n",
    "    \n",
    "    def choose_best_pairs(self, cand_source_words, cand_target_words, updates = None):\n",
    "        '''Find best matches for all and select top self.K'''\n",
    "        \n",
    "        if not updates:\n",
    "            updates = self.updates\n",
    "            \n",
    "        dist_scores = list()\n",
    "        \n",
    "        for source in cand_source_words:\n",
    "            sem_cands = [cand[0] for cand in self.get_sem_candidates(source, cand_target_words, K = self.K)]\n",
    "            # throwing away sem sim scores\n",
    "            dist_scores.append(self.find_best_match(source, sem_cands))\n",
    "        \n",
    "\n",
    "        best_pairs = sorted(dist_scores, key = lambda x:x[2])[:updates]\n",
    "        \n",
    "        return best_pairs\n",
    "    \n",
    "    \n",
    "    def get_batch(self, cand_source_words, batch_size = None):\n",
    "        '''Randomly choose batch based on pre-set batch size'''\n",
    "        if batch_size is None:\n",
    "            batch_size = self.batch_size\n",
    "        cand_source_words = list(cand_source_words.keys())\n",
    "        return random.choices(cand_source_words, k = self.batch_size)\n",
    "    \n",
    "    \n",
    "    def em_iterations(self, cand_source_words, cand_target_words):\n",
    "        \n",
    "        self.training = True\n",
    "        \n",
    "        for it in tqdm(range(self.iterations)):\n",
    "#             print(\"Running iteration: {}\".format(it+1))\n",
    "            \n",
    "            batch_source_words = self.get_batch(cand_source_words)\n",
    "            \n",
    "            # EXPECTATION\n",
    "#             print(\"E-Step\")\n",
    "            best_pairs = self.choose_best_pairs(batch_source_words, cand_target_words)\n",
    "            best_pairs = [(trip[0], trip[1]) for trip in best_pairs]\n",
    "            if it % 20 == 0 and it:\n",
    "                print(\"Words chosen for updates : \", best_pairs)\n",
    "            \n",
    "            # MAXIMISATION\n",
    "#             print(\"M-Step\")\n",
    "            self.update_params_all(best_pairs)\n",
    "            \n",
    "            # CHECK FOR CONVERGENCE\n",
    "#             if self.check_convergence():\n",
    "#                 print(\"Converged!\")\n",
    "#                 break\n",
    "            \n",
    "        self.training = False\n",
    "        \n",
    "    def dump_model_params(self, PARAMS_DIR):\n",
    "        '''Save model params for inspection and continued training'''\n",
    "        if not os.path.isdir(PARAMS_DIR):\n",
    "            os.makedirs(PARAMS_DIR)\n",
    "        \n",
    "        with open(PARAMS_DIR+\"trans_matrix.json\", \"w\") as f:\n",
    "            json.dump(self.trans_matrix, f, ensure_ascii = False, indent = 2)\n",
    "        \n",
    "        with open(PARAMS_DIR+\"totals.json\", \"w\") as f:\n",
    "            json.dump(self.totals, f, ensure_ascii = False, indent = 2)\n",
    "            \n",
    "            \n",
    "        with open(PARAMS_DIR+\"seen.json\", \"w\") as f:\n",
    "            seen = {source:list(target_set) for source, target_set in self.seen.items()}\n",
    "            json.dump(seen, f, ensure_ascii = False, indent = 2)\n",
    "            \n",
    "    def load_model_params(self, PARAMS_DIR):\n",
    "        '''Initialize by loading pre-trained params'''\n",
    "        with open(PARAMS_DIR+\"trans_matrix.json\", \"r\") as f:\n",
    "            self.trans_matrix = json.load(f)\n",
    "        \n",
    "        with open(PARAMS_DIR+\"totals.json\", \"r\") as f:\n",
    "            self.totals = json.load(f)\n",
    "            \n",
    "            \n",
    "        with open(PARAMS_DIR+\"seen.json\", \"r\") as f:\n",
    "            self.seen = json.load(f)\n",
    "            self.seen = {source:set(target_set) for source, target_set in self.seen.items()}\n",
    "        \n",
    "    \n",
    "\n",
    "    def build_lexicon(self, cand_source_words, cand_target_words, max_lexicon_length):\n",
    "        '''Build bilingual lexicon using current model parameters'''\n",
    "        \n",
    "        max_lexicon_length = min(max_lexicon_length, len(cand_source_words))\n",
    "        cand_source_words = defaultdict(lambda:0, \\\n",
    "                                        {w:f for w, f in Counter(cand_source_words).most_common(max_lexicon_length)})\n",
    "        \n",
    "        lexicon = defaultdict(lambda: dict())\n",
    "        \n",
    "        for source in cand_source_words:\n",
    "            sem_cands = [cand[0] for cand in self.get_sem_candidates(source, cand_target_words, K = self.K)]\n",
    "            _, top_k = self.find_best_match(source, sem_cands)\n",
    "            lexicon[source] = {target:-score for (target, score) in top_k}\n",
    "        \n",
    "        return lexicon\n",
    "        \n",
    "        \n",
    "    def save_lexicon(self, lexicon, OUTPATH):\n",
    "        '''Dump lexicon'''\n",
    "        OUTDIR = \"/\".join(OUTPATH.split(\"/\")[:-1])+\"/\"\n",
    "        if not os.path.isdir(OUTDIR):\n",
    "            os.makedirs(OUTDIR)\n",
    "\n",
    "        with open(OUTPATH, \"w\") as f:\n",
    "            json.dump(lexicon, f, ensure_ascii = False, indent = 2)\n",
    "\n",
    "\n",
    "    def driver(self, source_file, target_file, model, \\\n",
    "               max_lexicon_length = math.inf, min_source_freq = None, min_target_freq = None, \\\n",
    "               iterations = 50, batch_size = 100, updates = 20, \\\n",
    "               load_pretrained_model = False, \\\n",
    "               OUTPATH = None, PARAMS_DIR = None):\n",
    "\n",
    "        # Read files\n",
    "        source_corpus = self.read_file(source_file)\n",
    "        target_corpus = self.read_file(target_file)\n",
    "        \n",
    "        # Read model instance\n",
    "        if isinstance(model, str):\n",
    "            print(\"LOADING FASTTEXT\")\n",
    "            model = FastText(model)\n",
    "\n",
    "        # Filter\n",
    "        source_words = Counter(source_corpus.split())\n",
    "        target_words = Counter(target_corpus.split())\n",
    "        if min_source_freq is None:\n",
    "            min_source_freq = self.get_frequency_threshold(source_words)\n",
    "        if min_target_freq is None:\n",
    "#             min_target_freq = self.get_frequency_threshold(target_words)\n",
    "            min_target_freq = 0\n",
    "    \n",
    "        cand_source_words, cand_target_words = self.get_lexicon_words(source_words, target_words, \\\n",
    "                                                                      min_source_freq, min_target_freq)\n",
    "        \n",
    "\n",
    "        \n",
    "        # TODO: Add options for different types of initialization\n",
    "        self.initialize_model_params(iterations, batch_size, updates, \\\n",
    "                                     cand_source_words, cand_target_words, \\\n",
    "                                     load_pretrained_model, PARAMS_DIR)\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "        # TRAIN MODEL \n",
    "#         self.em_iterations(cand_source_words, cand_target_words)\n",
    "        \n",
    "        if PARAMS_DIR:\n",
    "            self.dump_model_params(PARAMS_DIR)\n",
    "\n",
    "\n",
    "        \n",
    "        # BUILD LEXICON\n",
    "        \n",
    "#         batch_source_words = self.get_batch(cand_source_words, 100)\n",
    "#         best_pairs = self.choose_best_pairs(batch_source_words, cand_target_words)\n",
    "#         print(best_pairs)\n",
    "        \n",
    "        print(\"BUILDING LEXICON\")\n",
    "        lexicon = self.build_lexicon(cand_source_words, cand_target_words, max_lexicon_length)\n",
    "\n",
    "        # Save lexicon\n",
    "        if OUTPATH:\n",
    "            self.save_lexicon(lexicon, OUTPATH)\n",
    "            \n",
    "        # Dump model parameters\n",
    "        \n",
    "        return lexicon\n",
    "            \n",
    "\n",
    "    def main(self):\n",
    "        args = self.get_args()\n",
    "        self.driver(args.source_file, args.target_file, args.model, \\\n",
    "        args.max_lexicon_length, args.min_source_freq, args.min_target_freq, \\\n",
    "        args.iterations, args.batch_size, args.updates, \\\n",
    "        args.load_pretrained_model, \\\n",
    "        args.OUTPATH, args.PARAMS_DIR)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "excess-knock",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR=\"../data/crawled_cleaned/\"\n",
    "anchor=\"hindi-urdu\"\n",
    "MODEL_DIR = \"../mli_sem_od/models/upsampled/\"\n",
    "PARAMS_DIR = \"model_params/\"\n",
    "iterations = 500\n",
    "min_source_freq = 5\n",
    "batch_size = 100\n",
    "updates = 10\n",
    "max_lexicon_length = 5000\n",
    "# gold_file = \"../evaluation_languages_home/eval_data/lexicons/hindi-urdu_source/hindi-urdu_bhojpuri.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "mechanical-condition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING rajasthani\n",
      "LOADING FASTTEXT\n",
      "BUILDING LEXICON\n",
      "PROCESSING nepali\n",
      "LOADING FASTTEXT\n",
      "BUILDING LEXICON\n",
      "PROCESSING hariyanvi\n",
      "LOADING FASTTEXT\n",
      "BUILDING LEXICON\n",
      "PROCESSING bhojpuri\n",
      "LOADING FASTTEXT\n",
      "BUILDING LEXICON\n",
      "PROCESSING maithili\n",
      "LOADING FASTTEXT\n",
      "BUILDING LEXICON\n",
      "PROCESSING brajbhasha\n",
      "LOADING FASTTEXT\n",
      "BUILDING LEXICON\n",
      "PROCESSING bajjika\n",
      "LOADING FASTTEXT\n",
      "BUILDING LEXICON\n",
      "PROCESSING magahi\n",
      "LOADING FASTTEXT\n",
      "BUILDING LEXICON\n",
      "PROCESSING angika\n",
      "LOADING FASTTEXT\n",
      "BUILDING LEXICON\n",
      "PROCESSING bhil\n",
      "LOADING FASTTEXT\n",
      "BUILDING LEXICON\n",
      "PROCESSING koraku\n",
      "LOADING FASTTEXT\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model_params/hindi-urdu_koraku/trans_matrix.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-58e56b602fef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLI_SEM_EMOD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     obj.driver(source_file, target_file, MODEL_PATH, \\\n\u001b[0m\u001b[1;32m     15\u001b[0m                      \u001b[0miterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_source_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_source_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                        \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-83-6bf142dfa0d9>\u001b[0m in \u001b[0;36mdriver\u001b[0;34m(self, source_file, target_file, model, max_lexicon_length, min_source_freq, min_target_freq, iterations, batch_size, updates, load_pretrained_model, OUTPATH, PARAMS_DIR)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;31m# TODO: Add options for different types of initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m         self.initialize_model_params(iterations, batch_size, updates, \\\n\u001b[0m\u001b[1;32m    370\u001b[0m                                      \u001b[0mcand_source_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcand_target_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m                                      load_pretrained_model, PARAMS_DIR)\n",
      "\u001b[0;32m<ipython-input-83-6bf142dfa0d9>\u001b[0m in \u001b[0;36minitialize_model_params\u001b[0;34m(self, iterations, batch_size, updates, cand_source_words, cand_target_words, load_pretrained_model, PARAMS_DIR)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mload_pretrained_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPARAMS_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_trans_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcand_source_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcand_target_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-83-6bf142dfa0d9>\u001b[0m in \u001b[0;36mload_model_params\u001b[0;34m(self, PARAMS_DIR)\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_model_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPARAMS_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;34m'''Initialize by loading pre-trained params'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPARAMS_DIR\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"trans_matrix.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrans_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model_params/hindi-urdu_koraku/trans_matrix.json'"
     ]
    }
   ],
   "source": [
    "for lang in langs:\n",
    "    if lang in [\"hindi-urdu\"]:\n",
    "        continue\n",
    "    \n",
    "    print(\"PROCESSING {}\".format(lang))\n",
    "    \n",
    "    source_file=DATADIR+anchor+\".txt\"\n",
    "    target_file=DATADIR+lang+\".txt\"\n",
    "    OUTPATH = \"lexicons_top5/{}_{}.json\".format(anchor, lang)\n",
    "    PARAMS_PATH = PARAMS_DIR + \"{}_{}/\".format(anchor, lang)\n",
    "    MODEL_PATH = MODEL_DIR + \"{}_{}.bin\".format(lang, anchor)\n",
    "\n",
    "    obj = MLI_SEM_EMOD()\n",
    "    obj.driver(source_file, target_file, MODEL_PATH, \\\n",
    "                     iterations = iterations, min_source_freq = min_source_freq, updates = updates,\\\n",
    "                       batch_size = batch_size, \\\n",
    "                     max_lexicon_length = max_lexicon_length, \\\n",
    "                       load_pretrained_model = True, \\\n",
    "                       OUTPATH = OUTPATH, PARAMS_DIR = PARAMS_PATH\n",
    "               \n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "golden-disability",
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = [\n",
    "\"rajasthani\",\n",
    "\"nepali\",\n",
    "# \"punjabi\",\n",
    "\"hariyanvi\",\n",
    "\"bhojpuri\",\n",
    "\"maithili\",\n",
    "\"brajbhasha\",\n",
    "\"bajjika\",\n",
    "\"magahi\",\n",
    "\"angika\",\n",
    "# \"gujarati\",\n",
    "# \"khadi_boli\",\n",
    "# \"sanskrit\",\n",
    "# \"hindi-urdu\",\n",
    "\"bhil\",\n",
    "\"koraku\",\n",
    "\"baiga\",\n",
    "\"nimaadi\",\n",
    "\"malwi\",\n",
    "\"marathi\",\n",
    "\"bhadavari\",\n",
    "\"himachali\",\n",
    "\"garwali\",\n",
    "\"kumaoni\",\n",
    "\"kannauji\",\n",
    "\n",
    "\"bundeli\",\n",
    "\"awadhi\",\n",
    "\"chattisgarhi\",\n",
    "\n",
    "\"pali\",\n",
    "\"sindhi\",\n",
    "# \"bangla\"\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "superb-puzzle",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "math domain error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-f7278b7c2ed1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: math domain error"
     ]
    }
   ],
   "source": [
    "math.log(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-maria",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
