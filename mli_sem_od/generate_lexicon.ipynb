{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "square-entry",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import math\n",
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "import editdistance\n",
    "import jaro\n",
    "from pyfasttext import FastText\n",
    "import sys\n",
    "sys.path.append(\"../evaluation_languages_home/eval_scripts/\")\n",
    "from eval import Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "interior-circular",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLI_SEM_OD:\n",
    "\n",
    "    # def __init__(self, source_lang, target_lang):\n",
    "    #     '''Initialize langs'''\n",
    "    #     self.source_lang = source_lang\n",
    "    #     self.target_lang = target_lang\n",
    "\n",
    "\n",
    "    def get_args(self):\n",
    "        parser = argparse.ArgumentParser(description = \"Build a lexicon from 2 corpora using bilingual embeddings and \\\n",
    "        orthographic distance.\")\n",
    "        parser.add_argument(\"--source_file\", type=str, default=None, required=True, help=\"Source corpus filepath\")\n",
    "        parser.add_argument(\"--target_file\", type=str, default=None, required=True, help=\"Target corpus filepath\")\n",
    "        parser.add_argument(\"--model\", type=str, default=None, required=True, help=\"fastText bilingual embeddings file\")\n",
    "        parser.add_argument(\"--max_lexicon_length\", type=int, default=math.inf, help=\"Maximum length of extracted lexicon\")\n",
    "        parser.add_argument(\"--min_source_freq\", type=int, default=None, help=\"Min freq of source side words\")\n",
    "        parser.add_argument(\"--min_target_freq\", type=int, default=None, help=\"Min freq of target side words\")\n",
    "        parser.add_argument(\"--OUTPATH\", type=str, help=\"Path for saving lexicon (JSON)\")\n",
    "\n",
    "        return parser.parse_args()\n",
    "\n",
    "    def read_file(self, filepath):\n",
    "        '''Reads text file and returns as string'''\n",
    "        with open(filepath, \"r\") as f:\n",
    "            return f.read()\n",
    "\n",
    "    def get_lexicon_words(self, source_words, target_words, max_lexicon_length, min_source_freq, min_target_freq=None):\n",
    "        '''Decide which source-side words will be in the lexicon'''\n",
    "\n",
    "        cand_source_words = defaultdict(lambda:0, {w:f for w, f in source_words.most_common(max_lexicon_length) if f >= min_source_freq})\n",
    "#         cand_target_words = defaultdict(lambda:0, {w:f for w, f in target_words.items() if f >= min_target_freq})\n",
    "\n",
    "        return cand_source_words, target_words\n",
    "\n",
    "    def get_frequency_threshold(self, lang_lexicon):\n",
    "        '''Logarithmic freq threshold'''\n",
    "\n",
    "        total = sum(lang_lexicon.values())\n",
    "        return math.log(total, 100) - 1\n",
    "\n",
    "    def od_match(self, word, cand_target_words, weighted = False):\n",
    "        '''Find best match using OD\n",
    "        Here, cand_target_words are (word, sim_score) pairs. If weighted is set, the sim_scores are used,\n",
    "        otherwise not.\n",
    "        '''\n",
    "#         vowel_range = list(range(2305, 2315)) + list(range(2317, 2325)) + list(range(2365, 2384))\n",
    "        bad_char_range = range(2364, 2367)\n",
    "\n",
    "        min_dist, best_word = 2, \"\"\n",
    "        for cand in cand_target_words:\n",
    "#             ned = editdistance.eval(word, cand)/max(len(word), len(cand))\n",
    "#             if ned < min_dist:\n",
    "#                 min_dist = ned\n",
    "#                 best_word = cand\n",
    "\n",
    "            score = jaro.jaro_winkler_metric(word, cand[0])\n",
    "            if weighted:\n",
    "                # weight by sim score\n",
    "                score *= cand[1]\n",
    "                \n",
    "            dist = 1 - score\n",
    "                \n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                best_word = cand[0]\n",
    "\n",
    "        return best_word, min_dist\n",
    "    \n",
    "    \n",
    "    def get_candidates(self, word, model, cand_target_words, K = 50):\n",
    "        '''Get semantics-based candidates for a word based on bilingual embeddings'''\n",
    "        nns = model.nearest_neighbors(word, k = K)\n",
    "#         nns = [(word, 1)] + nns\n",
    "        return [nn for nn in nns if nn[0] in cand_target_words]\n",
    "    \n",
    "\n",
    "    def build_lexicon(self, cand_source_words, cand_target_words, model):\n",
    "        '''Build bilingual lexicon using NED'''\n",
    "        lexicon = defaultdict(lambda: dict())\n",
    "\n",
    "        for word in cand_source_words:\n",
    "            candidates = self.get_candidates(word, model, cand_target_words)\n",
    "            best_word, score = self.od_match(word, candidates)\n",
    "            lexicon[word][best_word] = 1 - score\n",
    "\n",
    "        return lexicon\n",
    "\n",
    "    def save_lexicon(self, lexicon, OUTPATH):\n",
    "        '''Dump lexicon'''\n",
    "        OUTDIR = \"/\".join(OUTPATH.split(\"/\")[:-1])+\"/\"\n",
    "        if not os.path.isdir(OUTDIR):\n",
    "            os.makedirs(OUTDIR)\n",
    "\n",
    "        with open(OUTPATH, \"w\") as f:\n",
    "            json.dump(lexicon, f, ensure_ascii = False, indent = 2)\n",
    "\n",
    "\n",
    "    def driver(self, source_file, target_file, model, max_lexicon_length = math.inf, \\\n",
    "    min_source_freq = None, min_target_freq = None, OUTPATH = None):\n",
    "        # Read files\n",
    "        source_corpus = self.read_file(source_file)\n",
    "        target_corpus = self.read_file(target_file)\n",
    "        \n",
    "        # Can pass either fastText model or filepath\n",
    "        \n",
    "        if isinstance(model, str):\n",
    "            print(\"LOADING FASTTEXT\")\n",
    "            model = FastText(model)\n",
    "\n",
    "        # Filter\n",
    "        source_words = Counter(source_corpus.split())\n",
    "        target_words = Counter(target_corpus.split())\n",
    "\n",
    "        max_lexicon_length = min(max_lexicon_length, len(source_words))\n",
    "        if min_source_freq is None:\n",
    "            min_source_freq = self.get_frequency_threshold(target_words)\n",
    "\n",
    "        cand_source_words, cand_target_words = self.get_lexicon_words(source_words, target_words, \\\n",
    "        max_lexicon_length, min_source_freq)\n",
    "#         print(min_source_freq, min_target_freq)\n",
    "        # Build lexicon\n",
    "        lexicon = self.build_lexicon(cand_source_words, cand_target_words, model)\n",
    "\n",
    "        # Save lexicon\n",
    "        if OUTPATH:\n",
    "            self.save_lexicon(lexicon, OUTPATH)\n",
    "\n",
    "    def main(self):\n",
    "        args = self.get_args()\n",
    "        self.driver(args.source_file, args.target_file, args.model, \\\n",
    "        args.max_lexicon_length, args.min_source_freq, args.min_target_freq, \\\n",
    "        args.OUTPATH)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     obj = MLI_SEM_OD()\n",
    "#     obj.main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "exclusive-rotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = MLI_SEM_OD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "genetic-narrow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING FASTTEXT\n"
     ]
    }
   ],
   "source": [
    "lang = \"nepali\"\n",
    "anchor = \"hindi-urdu\"\n",
    "\n",
    "PATH = \"../data/crawled_cleaned/\"\n",
    "SOURCEFILE = PATH + anchor + \".txt\"\n",
    "TARGETFILE = PATH + lang + \".txt\"\n",
    "MODEL_PATH = \"models/joint/{}_{}.bin\".format(lang, anchor)\n",
    "OUTPATH = \"lexicons_K50_noself/{}_{}.json\".format(anchor, lang)\n",
    "\n",
    "\n",
    "obj.driver(SOURCEFILE, TARGETFILE, model = MODEL_PATH, max_lexicon_length= 5000, OUTPATH = OUTPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "christian-header",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_file = \"../evaluation_nep/eval_data/lexicons/{0}_source/{0}_{1}.json\".format(anchor, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "activated-working",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_eval = Evaluation()\n",
    "\n",
    "result = obj_eval.driver(gold_file, OUTPATH, eval_type = \"loose\", OUTPATH=\"here.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspended-cartoon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-coaching",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elegant-projector",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affecting-concert",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
